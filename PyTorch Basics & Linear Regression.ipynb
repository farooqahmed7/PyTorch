{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics: Tensors & Gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "- Its core, PyTorch is a library for processing tensors. \n",
    "- A **tensor** is a number, vector, matrix or any n-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "\n",
    "# Linux / Binder\n",
    "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Windows\n",
    "#!pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# MacOS\n",
    "# !pip install numpy torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PyTorch. \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number\n",
    "t1=torch.tensor(4.)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector.\n",
    "t2=torch.tensor([1.,2,3,4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch should have same datatypes, else it converts automatically to int form as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "t3=torch.tensor([[5.,6],[7,8],[9,10]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13.],\n",
       "         [13., 14., 15.]],\n",
       "\n",
       "        [[15., 16., 17.],\n",
       "         [17., 18., 19.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-Dimensional array.\n",
    "t4=torch.tensor([[[11,12,13],\n",
    "                 [13,14,15]],\n",
    "                [[15,16,17],\n",
    "                [17,18,19.]]])\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([4])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Tensor can have any number of dimensions.\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13.],\n",
       "         [13., 14., 15.]],\n",
       "\n",
       "        [[15., 16., 17.],\n",
       "         [17., 18., 19.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tensor operations and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor.\n",
    "x=torch.tensor(3.)\n",
    "w=torch.tensor(4., requires_grad=True)\n",
    "b=torch.tensor(5., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithemtic Operation\n",
    "y=w*x+b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From above, **y** is a tensorwith the value 3*4+5 =17. We can automatically compute the derivative of **y** w.r.t the tensors that have **requires_grad** set to **True** i.e w and b. This feature of PyTorch is called **autograd**(automatic gradients).\n",
    "\n",
    "- To compute the derivatives, we can invoke the **.backward** method on our result **y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Compute Derivatives we use .backward method. \n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Derivatives are important for Optimization Algorithms such as Gradient Discent.\n",
    "# Derivatives of a variable is stored in '.grad' property of respective tensors.\n",
    "\n",
    "# Display gradients.\n",
    "print('dy/dx:', x.grad) # since we didnt give requires_grad=True, it gives None for this derivative.\n",
    "print('dy/dw:',w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient is used when we deal with matrixes.\n",
    "# Derivative is used when we deal with numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor functions\n",
    "- Apart from arithmetic operations, torch module also contains many functions for creating and manipulating tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42., 42.],\n",
       "        [42., 42.],\n",
       "        [42., 42.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tensor with a fixed value for every element.\n",
    "t6=torch.full((3,2),42) # (3,2) is shape and 42 is every element in matrix should be 42\n",
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.],\n",
       "        [42., 42.],\n",
       "        [42., 42.],\n",
       "        [42., 42.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating two tensors wwith compatible shapes.\n",
    "t7=torch.cat((t3,t6))\n",
    "t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9589, -0.2794],\n",
       "        [ 0.6570,  0.9894],\n",
       "        [ 0.4121, -0.5440],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing sine of every element.\n",
    "t8=torch.sin(t7)\n",
    "t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9589, -0.2794],\n",
       "         [ 0.6570,  0.9894]],\n",
       "\n",
       "        [[ 0.4121, -0.5440],\n",
       "         [-0.9165, -0.9165]],\n",
       "\n",
       "        [[-0.9165, -0.9165],\n",
       "         [-0.9165, -0.9165]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing shape of tensor\n",
    "t9= t8.view(3,2,2)\n",
    "t9\n",
    "# or\n",
    "# t9= t8.reshape(3,2,2)\n",
    "\n",
    "# For above both syntaxes are same for reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability with Numpy.\n",
    "- **Numpy** is one of the popular open-source library used for mathematical and scientific computing in Python.It enables efficient operations on large multi-dimensional arrays and has a vast ecosystem of supporting libraries which includes:\n",
    "  - **Pandas** for file I/O and data analysis.\n",
    "  - **Matplotlib** for plotting and visualization.\n",
    "  - **OpenCV** for image and video processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[3,4,5,6]\n",
    "arr=np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting numpy array to PyTorch tensors.\n",
    "tensor=torch.from_numpy(arr)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 12.])\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor([2,3])\n",
    "y=torch.Tensor([3,4])\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.zeros([3,5])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1584, 0.6196, 0.2311, 0.2726, 0.8147, 0.6848],\n",
       "        [0.5885, 0.7764, 0.6482, 0.4638, 0.2022, 0.9913],\n",
       "        [0.0875, 0.0905, 0.2957, 0.2882, 0.0979, 0.1427],\n",
       "        [0.5846, 0.3798, 0.7731, 0.1545, 0.2389, 0.0415],\n",
       "        [0.1900, 0.6544, 0.0590, 0.7609, 0.7562, 0.6265]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=torch.rand([5,6])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1584, 0.6196, 0.2311, 0.2726, 0.8147, 0.6848, 0.5885, 0.7764, 0.6482,\n",
       "         0.4638],\n",
       "        [0.2022, 0.9913, 0.0875, 0.0905, 0.2957, 0.2882, 0.0979, 0.1427, 0.5846,\n",
       "         0.3798],\n",
       "        [0.7731, 0.1545, 0.2389, 0.0415, 0.1900, 0.6544, 0.0590, 0.7609, 0.7562,\n",
       "         0.6265]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping an array\n",
    "s.view([3,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Steps in Linear Regression.\n",
    "- Design the model(input , output size, forward pass) \n",
    "- Construct loss and optimizer\n",
    "- Training loop\n",
    "  - Forward pass: compute predictions and loss\n",
    "  - backward pass: gradients\n",
    "  - update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "In this tutorial, we'll discuss one of the foundational algorithms in machine learning: *Linear regression*. We'll create a model that predicts crop yields for apples and oranges (*target variables*) by looking at the average temperature, rainfall, and humidity (*input variables or features*) in a region. Here's the training data:\n",
    "\n",
    "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "```\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "```\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The learning part of linear regression is to figure out a set of weights **w11, w12... w23, b1, b2** using the training data to make accurate predictions for new data. The **learned** weights will be used to predict the  yeilds for apples and oranges in a new region using the average temperature, rainfall and humidity for that region.\n",
    "\n",
    "\n",
    "-  We train the model by adjusting the weights slightly many times to make better predicitons, using an optimization technique called **gradient descent** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  data\n",
    "- We can represent the training data using two metrices: **inputs** and **targets**, each with one row per observation and one column per variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73,67,43],\n",
    "                [91,88,64],\n",
    "                [87,134,58],\n",
    "                [102,43,37],\n",
    "                [69,96,70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variables.(apples,oranges)\n",
    "targets = np.array([[56,70],\n",
    "                 [81,110],\n",
    "                 [119,133],\n",
    "                 [22,37],\n",
    "                 [103,119]],dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've separated the input and target variables because we'll operate on them separately. Also we've created numpy arrays, because this is typically how we train the data\n",
    "-  Other method is ----> read some CSV files as numpy arrays, do some processing, and then convert them to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numpy and pytorch into tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 110.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model from scratch.\n",
    "- The weights and biases (**w11, w12,... w23, b1 & b2**) can also be represented as matrices, initialized as random values. The first row of **w** and the first element of **b** are used to predict the first target variable, i.e., yield of apples, and similarly, the second for oranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6787,  0.4869, -1.0340],\n",
      "        [ 0.6957,  0.1356,  0.5034]], requires_grad=True)\n",
      "tensor([-0.8766,  0.8002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Adding 'weights and biases' randomly.\n",
    "\n",
    "w=torch.randn(2,3, requires_grad=True)\n",
    "b=torch.randn(2, requires_grad=True)\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch_randn** creates a tensor with the given shape with elements picked randomly from a normal distribution with mean as 0 and standard deviation 1.\n",
    "- Our model is just does matrix multiplication between **inputs** and the **weights** (transposed) and adds the bias **b**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "    # x will be inputs\n",
    "    # @ is matrix multiplication\n",
    "    #.t() is transpose\n",
    "    # b is biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **@** is matrix multiplication in PyTorch and **.t** returns the transpose of a tensor.\n",
    "-  The matrix obtained by passing the input data into the model is a set of predictions for the target variables.\n",
    "- math: y= xA^T + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Steps.\n",
    "- Generate Predictions\n",
    "- Calculate the loss\n",
    "- Compute gradients w.r.t the weights and biases\n",
    "- Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "- Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 36.8331,  82.3233],\n",
      "        [ 37.5616, 108.2666],\n",
      "        [ 63.4505, 108.7023],\n",
      "        [ 51.0338,  96.2239],\n",
      "        [ 20.3208,  97.0659]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions.\n",
    "preds=model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 110.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Comparing our predictions with targets.\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  When comparing the actual values to predicted, there are differences between them. Because we have initialized our model with random weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function.\n",
    "- To evaluate our model by comparing the model's prediction with the actual targets.(preds-targets)\n",
    "- Calculate the differences between the two matrics(preds & targets)\n",
    "- Square all elements of the difference matrix **to remove negative values**.\n",
    "- Calculate the average of the elements in the resulting matrix. The result is single number known as the **mean squared error**(MSE)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "diff=preds-targets\n",
    "diff_sqr= diff*diff # provides element wise multiplication.\n",
    "torch.sum(diff_sqr)/diff.numel() # To take average or mean of those values.\n",
    "\n",
    "#'.numel()' gives total number of elements in the particular variable or set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss.\n",
    "def MSE(x,y): # This is Mean Squared Error.\n",
    "    diff= x-y\n",
    "    return torch.sum (diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1775.2646, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss=MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradients.\n",
    "- In PyTorch we can compute the gradient or derivative of the loss w.r.t the weights and biases since we have **requires_grad=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2585.6597, -3847.8286, -2307.8794],\n",
      "        [  631.0690,  -428.3635,   -66.8811]])\n",
      "tensor([-34.3600,   4.7164])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These gradients are stored in **.grad** property of the respective tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6787,  0.4869, -1.0340],\n",
      "        [ 0.6957,  0.1356,  0.5034]], requires_grad=True)\n",
      "tensor([[-2585.6597, -3847.8286, -2307.8794],\n",
      "        [  631.0690,  -428.3635,   -66.8811]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights.\n",
    "print(w)\n",
    "# Since w is a matrix then .grad of w will also be a matrix.\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8766,  0.8002], requires_grad=True)\n",
      "tensor([-34.3600,   4.7164])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust weights and biases to reduce the loss\n",
    "- Loss(MSE) is a quadratic function of our weights and biases, our main aim is **to find the set of weights where the loss is the lowest**. If we plot a graph of the loss w.r.t any individual weight or bias element, it looks like below figure.\n",
    "- Gradient indicates the rate of change of the loss, i.e the slope of the loss function w.r.t the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If a gradient element is **positive**:\n",
    "\n",
    "* **increasing** the weight element's value slightly will **increase** the loss\n",
    "* **decreasing** the weight element's value slightly will **decrease** the loss\n",
    "\n",
    "![postive-gradient](https://i.imgur.com/WLzJ4xP.png)\n",
    "\n",
    "If a gradient element is **negative**:\n",
    "\n",
    "* **increasing** the weight element's value slightly will **decrease** the loss\n",
    "* **decreasing** the weight element's value slightly will **increase** the loss\n",
    "\n",
    "![negative=gradient](https://i.imgur.com/dvG2fxU.png)\n",
    "\n",
    "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of _the gradient descent_ optimization algorithm that we'll use to improve our model (by _descending_ along the _gradient_).\n",
    "\n",
    "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We reset the gradients to zero by calling **.zero_()** method because everytime when we call .backward PyTorch keeps adding the gradients values into w.grad, since there is single variable to capture all gradients. Hence everytime when we done with gradient related work we need to **clear out the gradients** by setting gradient back to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 36.8331,  82.3233],\n",
      "        [ 37.5616, 108.2666],\n",
      "        [ 63.4505, 108.7023],\n",
      "        [ 51.0338,  96.2239],\n",
      "        [ 20.3208,  97.0659]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss.\n",
    "preds=model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1775.2646, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss.\n",
    "loss=MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2585.6597, -3847.8286, -2307.8794],\n",
      "        [  631.0690,  -428.3635,   -66.8811]])\n",
      "tensor([-34.3600,   4.7164])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since our gradient element is positive we need to subtract the small quantity propotional to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the weights by subtracting a small quantity proportional to the gradient.\n",
    "# Reseting the gradients to zero.\n",
    "with torch.no_grad():\n",
    "    w-= w.grad*1e-3 # For each weight element, we will subtract small quantity proportional to the corresponding gradient element, which does this element wise for entire matrix.\n",
    "    b-= b.grad*1e-3\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.no_grad() is to indicate to PyTorch that we should'nt track, calculate or modify gradients while updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2644, 4.3348, 1.2738],\n",
      "        [0.0647, 0.5640, 0.5703]], requires_grad=True)\n",
      "tensor([-0.8423,  0.7955], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# New Weights and biases.\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  With these new weights and biases, need to calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[582.6640,  67.8268],\n",
      "        [759.2042,  92.8110],\n",
      "        [937.9033, 115.0744],\n",
      "        [565.6537,  52.7444],\n",
      "        [729.7089,  99.3220]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds=model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(209751.8438, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss=MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Reduce the loss further:\n",
    "- To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. each iteration is called as **Epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    preds=model(inputs)\n",
    "    loss=MSE(preds,targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w-=w.grad*1e-5\n",
    "        b-=b.grad*1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, verifying the loss in lower order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(136.7434, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds=model(inputs)\n",
    "loss=MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 59.9405,  74.1622],\n",
       "        [ 74.2641, 102.3236],\n",
       "        [132.2929, 132.4305],\n",
       "        [ 37.0775,  51.0612],\n",
       "        [ 78.5946, 113.2918]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 110.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regresson using PyTorch built-ins\n",
    "-  The model and training process above executed were using basic matrix operations. But since this is common pattern, PyTorch has several built-in functions and classes to make it easy to create and train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We are using  training examples time time, to illustrate how to work with large datasets in small batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "- we create **TensorDataset**, which allows access to rows from **inputs** and **targets** as tuples and provides standard APIs for  working with many different types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.]]), tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.]]))\n"
     ]
    }
   ],
   "source": [
    "# Defining Dataset\n",
    "train_ds=TensorDataset(inputs, targets)\n",
    "print(train_ds[0:3]) # Picks first 3 rows of both input and output data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All the datasets either images, text, tabular data, etc. will be converted into pytorch datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This **TensorDataset** allows us to access a small section of  the training data using the array indexing notations([:3] in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "- We also create a **DataLoader**, which can **split the data into batches of a predefined size** while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001FE5FACD0B8>\n"
     ]
    }
   ],
   "source": [
    "# Define data loader\n",
    "\n",
    "batch_size=5 # deals the data with batches of 5\n",
    "train_dl=DataLoader(train_ds, batch_size, shuffle=True)\n",
    "print(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.,  44.,  37.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [103.,  43.,  36.]])\n",
      "tensor([[ 21.,  38.],\n",
      "        [ 56.,  70.],\n",
      "        [118., 134.],\n",
      "        [ 81., 101.],\n",
      "        [ 20.,  38.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb) # Looking for one batch of data after shuffling.\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In each iteration, the data lloader returns one batch of the data with the given batch size. If **shuffle** is set to **True**, It shuffles the training data before creating batches. Shuffling even helps us po randomize the input to the optimization algorithm, leadinig to a faster reduction in the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "- Instead of initializing the weights and biases namually, we can define the model using the **nn.Linear** class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5020,  0.1231, -0.3146],\n",
      "        [ 0.4185, -0.4518, -0.2985]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5296, -0.0403], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model=nn.Linear(3,2) # Specifying '3' inputs and '2' outputs i.e \n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch models has a helpful **.parameters** method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5020,  0.1231, -0.3146],\n",
       "         [ 0.4185, -0.4518, -0.2985]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.5296, -0.0403], requires_grad=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters i.e list of weights and biases.\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 31.8923, -12.5903],\n",
       "        [ 36.9057, -20.8121],\n",
       "        [ 42.4495, -41.4766],\n",
       "        [ 45.3820,  12.1810],\n",
       "        [ 24.9597, -35.4254],\n",
       "        [ 32.2712, -11.7200],\n",
       "        [ 36.4679, -20.6589],\n",
       "        [ 42.6368, -41.3565],\n",
       "        [ 45.0032,  11.3107],\n",
       "        [ 24.1431, -36.1424],\n",
       "        [ 31.4546, -12.4371],\n",
       "        [ 37.2845, -19.9418],\n",
       "        [ 42.8873, -41.6299],\n",
       "        [ 46.1986,  12.8981],\n",
       "        [ 24.5809, -36.2957]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "preds=model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- Instead of defining the loss function manually, we can use built-in loss function **mse_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing nn.functional. This package contains many useful loss functions.\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the documentation help during the coding i,e helps to get clarify the syntaxes\n",
    "?nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9156.9521, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss=loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- Instead of manually manipulating the model's weight and biases using gradients, we can use the optimizer **optim.SGD**. SGD is for 'Stochastic Gradient Descent'. The term Stoochastic indicates that samples are selected in random batches instead of a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **model.parameters()** is passed as an argument to **optim.SGD**, so that the optimizer knows which matrices should be modified during the update setp. Even we can specify a **learning rate which controls the amount by which the parameters are modified**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Steps involved to implement gradient descent are:\n",
    "- Generate Predictions\n",
    "- Calculate the loss\n",
    "- Compute gradients w.r.t the weights and biases\n",
    "- Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "The only change is that we'll work batches of the data instead of processing the entire training data in every iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred=model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss=loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step() # This updates weights, bias everything\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad() # Resets all the gradients back to zero\n",
    "            \n",
    "        # Print the progress\n",
    "        if (epoch+1) %10==0:\n",
    "            print('Epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# For every 10 epochs we are getting loss value, then we are formatting and printing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], loss:1453.8254\n",
      "Epoch [20/100], loss:200.2012\n",
      "Epoch [30/100], loss:146.2127\n",
      "Epoch [40/100], loss:429.0978\n",
      "Epoch [50/100], loss:212.6598\n",
      "Epoch [60/100], loss:132.7685\n",
      "Epoch [70/100], loss:77.0179\n",
      "Epoch [80/100], loss:88.4021\n",
      "Epoch [90/100], loss:72.7909\n",
      "Epoch [100/100], loss:35.9717\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 59.0136,  72.5083],\n",
       "        [ 78.8558,  97.9103],\n",
       "        [122.6967, 135.7258],\n",
       "        [ 31.3636,  49.3879],\n",
       "        [ 90.2411, 107.1066],\n",
       "        [ 57.9182,  71.6338],\n",
       "        [ 78.0105,  97.3925],\n",
       "        [122.6663, 136.0674],\n",
       "        [ 32.4590,  50.2624],\n",
       "        [ 90.4912, 107.4633],\n",
       "        [ 58.1683,  71.9905],\n",
       "        [ 77.7604,  97.0358],\n",
       "        [123.5420, 136.2436],\n",
       "        [ 31.1136,  49.0312],\n",
       "        [ 91.3366, 107.9811]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "preds=model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55.0223, 69.3745]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the External data.\n",
    "model(torch.tensor([[75,63,44.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Predicted yeild of apples is 55.84 tons per hectare and that of oranges is 69.36 tons per hectare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
